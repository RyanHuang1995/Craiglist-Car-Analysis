{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Dependencies\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from splinter import Browser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = (\"../Car price data/accord_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Splinter Scraping data\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "# URL of page to be scraped\n",
    "url = 'https://losangeles.craigslist.org/search/cto?min_price=2000&auto_make_model=accord&min_auto_year=2008&max_auto_year=2018&min_auto_miles=0&max_auto_miles=1000000&condition=10&condition=20&condition=30&condition=40'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - - - - - - - - - - - Scraping New Page Starts!! - - - - - - - - - \n",
      "'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "# Scrape the data based on post info\n",
    "browser.visit(url)\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,\"html.parser\")\n",
    "next_page = soup.find(\"a\",class_ = \"button next\")[\"href\"]\n",
    "\n",
    "titles = []\n",
    "prices = []\n",
    "links = []\n",
    "car_data = []\n",
    "\n",
    "while next_page:\n",
    "    print(\" - - - - - - - - - - - Scraping New Page Starts!! - - - - - - - - - \")\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html,\"html.parser\")\n",
    "    results = soup.find_all('li', class_=\"result-row\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for result in results:\n",
    "    # Error handling\n",
    "        try:\n",
    "            # Identify and return title of listing\n",
    "            title = result.find('a', class_=\"result-title\").text\n",
    "            # Identify and return price of listing\n",
    "            price = result.a.span.text\n",
    "            # Identify and return link to listing\n",
    "            link = result.a['href']\n",
    "\n",
    "            # Print results only if title, price, and link are available\n",
    "            if (title and price and link):\n",
    "                titles.append(title)\n",
    "                prices.append(price)\n",
    "                links.append(link)\n",
    "                \n",
    "                post_link = result.a[\"href\"]\n",
    "                response2 = requests.get(post_link)\n",
    "                soup2 = BeautifulSoup(response2.text,\"html.parser\")\n",
    "                attributes = soup2.find_all(\"p\",class_=\"attrgroup\")\n",
    "                attribute_list = attributes[1].find_all(\"span\")\n",
    "                attr_dic = {}\n",
    "                \n",
    "                try:\n",
    "                \n",
    "                    tag = attributes[0].find(\"span\").find(\"b\").text.split(\" \")\n",
    "                \n",
    "                    yearlist = [str(num) for num in list(range(1900, 2020))]\n",
    "                    for word in tag:\n",
    "                        if word in yearlist:\n",
    "                            year = word\n",
    "                            for attr in attribute_list:\n",
    "                                try:\n",
    "                                    key,value = attr.text.split(\":\")\n",
    "                                    attr_dic[key] = value\n",
    "                                    attr_dic[\"Year\"] = year\n",
    "                                except ValueError:\n",
    "                                    pass\n",
    "                            if word not in tag:\n",
    "                                attr_dic[\"Year\"] = \"NA\"\n",
    "                    \n",
    "                    \n",
    "                except:\n",
    "                    attr_dic[\"Year\"] = \"NA\"\n",
    "                \n",
    "    \n",
    "                car_data.append(attr_dic)\n",
    "                \n",
    "                \n",
    "        except AttributeError as e:\n",
    "            print(e)\n",
    "    try:\n",
    "        browser.click_link_by_partial_text('next')\n",
    "        next_page = soup.find(\"a\",class_ = \"button next\")[\"href\"]\n",
    "    except:\n",
    "        print(\"- - - - - - - - - This is the last page we can find!!!- - - - - -\")\n",
    "        next_page = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store info as dataframe\n",
    "info_df = pd.DataFrame({\n",
    "    \"Info\" : titles,\n",
    "    \"Price\" : prices,\n",
    "    \"links\" : links\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show full dataframe\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "# Merge two dataframe\n",
    "car_df = pd.DataFrame(car_data)\n",
    "car_df[\"Title\"] = info_df[\"Info\"]\n",
    "car_df[\"Price\"] = info_df[\"Price\"]\n",
    "car_df[\"Link\"] = info_df[\"links\"]\n",
    "car_df[\"Make\"] = \"Honda\"\n",
    "car_df[\"Model\"] = \"Accord\"\n",
    "car_df = car_df[['Title','Make', 'Model','Year', 'Condition', 'Cylinders', 'Drive', 'Fuel', 'Odometer',\n",
    "       'Paint_color', 'Size', 'Title_status', 'Transmission', 'Type', \n",
    "       'Price', 'VIN', 'Link']]\n",
    "car_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out put as CSV file\n",
    "car_df.to_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
